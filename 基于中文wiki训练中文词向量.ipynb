{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本词向量利用的是中文维基百科的语料进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料地址：[Link](https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2)（大小1.16G）\n",
    "也可以在我的网盘上下载：\n",
    "链接: [Pan](https://pan.baidu.com/s/16eS2730jyIZuLvpO0ZLV_w) 提取码: ihu4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原数据的格式是xml，我们要将其转换为txt。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用的是`gensim`自带的WikiCorpus，首先读取xml文件到`input_file`中，然后其中的`get_texts`方法会生成一个迭代器，每一个迭代蕴含了一篇文章，这样我们就可以将其写入新的txt文件中了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Wiki data reading...\n",
      "Chinese Wiki data reading finishes.\n",
      "Transformation begins...\n",
      "#10000 of texts have been processed.\n",
      "#20000 of texts have been processed.\n",
      "#30000 of texts have been processed.\n",
      "#40000 of texts have been processed.\n",
      "#50000 of texts have been processed.\n",
      "#60000 of texts have been processed.\n",
      "#70000 of texts have been processed.\n",
      "#80000 of texts have been processed.\n",
      "#90000 of texts have been processed.\n",
      "#100000 of texts have been processed.\n",
      "#110000 of texts have been processed.\n",
      "#120000 of texts have been processed.\n",
      "#130000 of texts have been processed.\n",
      "#140000 of texts have been processed.\n",
      "#150000 of texts have been processed.\n",
      "#160000 of texts have been processed.\n",
      "#170000 of texts have been processed.\n",
      "#180000 of texts have been processed.\n",
      "#190000 of texts have been processed.\n",
      "#200000 of texts have been processed.\n",
      "#210000 of texts have been processed.\n",
      "#220000 of texts have been processed.\n",
      "#230000 of texts have been processed.\n",
      "#240000 of texts have been processed.\n",
      "#250000 of texts have been processed.\n",
      "Transformation finished.\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "input_path = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "output_path = 'zhwiki.txt'\n",
    "print('Chinese Wiki data reading...')\n",
    "input_file = WikiCorpus(input_path, lemmatize=False, dictionary={})\n",
    "print('Chinese Wiki data reading finishes.')\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    print('Transformation begins...')\n",
    "    count = 0\n",
    "    for text in input_file.get_texts():\n",
    "        output_file.write(' '.join(text) + '\\n')\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been processed.\")\n",
    "    print('Transformation finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、繁体数据转换为简体数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该Wiki数据是繁体中文数据，我们要把他们转换为简体中文数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Chinese to Simplified Chinese.\n",
      "Traditional Chinese file reading...\n",
      "Traditional Chinese file reading finishes...\n",
      "Tradition to simplified begins...\n",
      "#10000 of texts have been transformed.\n",
      "#20000 of texts have been transformed.\n",
      "#30000 of texts have been transformed.\n",
      "#40000 of texts have been transformed.\n",
      "#50000 of texts have been transformed.\n",
      "#60000 of texts have been transformed.\n",
      "#70000 of texts have been transformed.\n",
      "#80000 of texts have been transformed.\n",
      "#90000 of texts have been transformed.\n",
      "#100000 of texts have been transformed.\n",
      "#110000 of texts have been transformed.\n",
      "#120000 of texts have been transformed.\n",
      "#130000 of texts have been transformed.\n",
      "#140000 of texts have been transformed.\n",
      "#150000 of texts have been transformed.\n",
      "#160000 of texts have been transformed.\n",
      "#170000 of texts have been transformed.\n",
      "#180000 of texts have been transformed.\n",
      "#190000 of texts have been transformed.\n",
      "#200000 of texts have been transformed.\n",
      "#210000 of texts have been transformed.\n",
      "#220000 of texts have been transformed.\n",
      "#230000 of texts have been transformed.\n",
      "#240000 of texts have been transformed.\n",
      "#250000 of texts have been transformed.\n",
      "Tradition to simplified finished.\n"
     ]
    }
   ],
   "source": [
    "import zhconv\n",
    "print('Traditional Chinese to Simplified Chinese.')\n",
    "input_path = 'zhwiki.txt'\n",
    "output_path = 'zhwiki.simplify.txt'\n",
    "with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "    print('Traditional Chinese file reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('Traditional Chinese file reading finishes...')\n",
    "print('Tradition to simplified begins...')\n",
    "count = 0\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    for line in lines:\n",
    "        output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been transformed.\")\n",
    "print('Tradition to simplified finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用结巴分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Chinese wiki data reading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Chinese wiki data reading finishes.\n",
      "Tokenization begins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.747 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#10000 of texts have been tokenized.\n",
      "#20000 of texts have been tokenized.\n",
      "#30000 of texts have been tokenized.\n",
      "#40000 of texts have been tokenized.\n",
      "#50000 of texts have been tokenized.\n",
      "#60000 of texts have been tokenized.\n",
      "#70000 of texts have been tokenized.\n",
      "#80000 of texts have been tokenized.\n",
      "#90000 of texts have been tokenized.\n",
      "#100000 of texts have been tokenized.\n",
      "#110000 of texts have been tokenized.\n",
      "#120000 of texts have been tokenized.\n",
      "#130000 of texts have been tokenized.\n",
      "#140000 of texts have been tokenized.\n",
      "#150000 of texts have been tokenized.\n",
      "#160000 of texts have been tokenized.\n",
      "#170000 of texts have been tokenized.\n",
      "#180000 of texts have been tokenized.\n",
      "#190000 of texts have been tokenized.\n",
      "#200000 of texts have been tokenized.\n",
      "#210000 of texts have been tokenized.\n",
      "#220000 of texts have been tokenized.\n",
      "#230000 of texts have been tokenized.\n",
      "#240000 of texts have been tokenized.\n",
      "#250000 of texts have been tokenized.\n",
      "Tokenization finished.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "input_path = 'zhwiki.simplify.txt'\n",
    "output_path = 'zhwiki.simplify.tok.txt'\n",
    "with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "    print('Simplified Chinese wiki data reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('Simplified Chinese wiki data reading finishes.')\n",
    "print('Tokenization begins.')\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been tokenized.\")\n",
    "print('Tokenization finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、去除非中文词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些词语中会包含非中文的词，我们要利用正则表达式将该词去除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Chinese wiki data reading...\n",
      "Simplified Chinese wiki data reading finishes.\n",
      "Remove Non-zh begins...\n",
      "#10000 of texts have been processed.\n",
      "#20000 of texts have been processed.\n",
      "#30000 of texts have been processed.\n",
      "#40000 of texts have been processed.\n",
      "#50000 of texts have been processed.\n",
      "#60000 of texts have been processed.\n",
      "#70000 of texts have been processed.\n",
      "#80000 of texts have been processed.\n",
      "#90000 of texts have been processed.\n",
      "#100000 of texts have been processed.\n",
      "#110000 of texts have been processed.\n",
      "#120000 of texts have been processed.\n",
      "#130000 of texts have been processed.\n",
      "#140000 of texts have been processed.\n",
      "#150000 of texts have been processed.\n",
      "#160000 of texts have been processed.\n",
      "#170000 of texts have been processed.\n",
      "#180000 of texts have been processed.\n",
      "#190000 of texts have been processed.\n",
      "#200000 of texts have been processed.\n",
      "#210000 of texts have been processed.\n",
      "#220000 of texts have been processed.\n",
      "#230000 of texts have been processed.\n",
      "#240000 of texts have been processed.\n",
      "#250000 of texts have been processed.\n",
      "Remove Non-zh finishes.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_path = 'zhwiki.simplify.tok.txt'\n",
    "output_path = 'zhwiki.data.txt'\n",
    "with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "    print('Simplified Chinese wiki data reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('Simplified Chinese wiki data reading finishes.')\n",
    "print('Remove Non-zh begins...')\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    count = 0\n",
    "    remove = r'^[\\u4e00-\\u9fa5]+$'\n",
    "    for line in lines:\n",
    "        line_list = line.split('\\n')[0].split(' ')\n",
    "        new_line = []\n",
    "        for word in line_list:\n",
    "            if re.search(remove, word):\n",
    "                new_line.append(word)\n",
    "        output_file.write(' '.join(new_line) + '\\n')\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been processed.\")\n",
    "print('Remove Non-zh finishes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Generation begin...\n",
      "Word2Vec Generation finishes.\n",
      "Model Saving...\n",
      "Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "input_path = 'zhwiki.data.txt'\n",
    "output_path = 'zhwiki.model'\n",
    "print('Word2Vec Generation begin...')\n",
    "model = Word2Vec(LineSentence(input_path),\n",
    "                 size=300,\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=multiprocessing.cpu_count())\n",
    "print('Word2Vec Generation finishes.')\n",
    "print('Model Saving...')\n",
    "model.save(output_path)\n",
    "print('Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('zhwiki.model.vector', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "new_model = gensim.models.KeyedVectors.load_word2vec_format('zhwiki.model.vector',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('轿车', 0.6613024473190308),\n",
       " ('卡车', 0.6038083434104919),\n",
       " ('商用车', 0.6000862717628479),\n",
       " ('摩托车', 0.599767804145813),\n",
       " ('雪佛兰', 0.5972775220870972)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.similar_by_word('汽车', topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
